// Website Scraper for CFLAR Knowledge Base
// Run with: npm run scrape

import fetch from 'node-fetch';
import * as cheerio from 'cheerio';
import { writeFileSync } from 'fs';
import { fileURLToPath } from 'url';
import { dirname, join } from 'path';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const BASE_URL = 'https://cflar.dream.press';

// Pages to scrape
const PAGES_TO_SCRAPE = [
  '/',
  '/about',
  '/tours',
  '/donate',
  '/volunteer',
  '/contact',
  '/animals',
  '/education',
  '/events',
];

async function scrapePage(url) {
  try {
    console.log(`Scraping: ${url}`);
    
    const response = await fetch(url);
    if (!response.ok) {
      console.log(`  âš ï¸  Failed to fetch ${url}: ${response.status}`);
      return null;
    }

    const html = await response.text();
    const $ = cheerio.load(html);

    // Remove scripts, styles, and other non-content elements
    $('script, style, nav, header, footer, iframe, noscript').remove();

    // Extract main content
    const title = $('title').text().trim() || $('h1').first().text().trim();
    const mainContent = $('main').length 
      ? $('main').text() 
      : $('body').text();

    // Clean up the text
    const cleanText = mainContent
      .replace(/\s+/g, ' ')
      .replace(/\n+/g, '\n')
      .trim();

    // Extract key information
    const headings = [];
    $('h1, h2, h3').each((i, el) => {
      const text = $(el).text().trim();
      if (text) headings.push(text);
    });

    console.log(`  âœ“ Successfully scraped: ${title}`);

    return {
      url,
      title,
      content: cleanText.substring(0, 5000), // Limit content length
      headings: headings.slice(0, 10),
      scrapedAt: new Date().toISOString(),
    };
  } catch (error) {
    console.error(`  âœ— Error scraping ${url}:`, error.message);
    return null;
  }
}

async function scrapeWebsite() {
  console.log('ğŸ” Starting CFLAR website scrape...\n');

  const knowledgeBase = [];

  for (const path of PAGES_TO_SCRAPE) {
    const fullUrl = `${BASE_URL}${path}`;
    const pageData = await scrapePage(fullUrl);
    
    if (pageData) {
      knowledgeBase.push(pageData);
    }

    // Be polite - wait between requests
    await new Promise(resolve => setTimeout(resolve, 1000));
  }

  // Save to public directory
  const outputPath = join(__dirname, '../lib/knowledge-base-data.ts');
  
  const output = {
    lastUpdated: new Date().toISOString(),
    baseUrl: BASE_URL,
    pageCount: knowledgeBase.length,
    pages: knowledgeBase,
  };

  // Generate TypeScript file content
  const tsContent = `// CFLAR Knowledge Base Data
// Auto-generated by scrape-website.js
// Last updated: ${output.lastUpdated}

export const knowledgeBaseData = ${JSON.stringify(output, null, 2)};
`;

  writeFileSync(outputPath, tsContent);

  console.log(`\nâœ… Scraping complete!`);
  console.log(`ğŸ“ Scraped ${knowledgeBase.length} pages`);
  console.log(`ğŸ’¾ Saved to: ${outputPath}`);
  console.log(`\nKnowledge base is ready to use!`);
}

// Run the scraper
scrapeWebsite().catch(console.error);